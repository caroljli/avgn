{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Syllabification\n",
    "- Takes the WAV datasets generated by '1.0-segment-song-from-wav-files' and segments into spectograms of syllables.\n",
    "- Outputs HDF5 file containing metadata about the cowbird singing, the syllables sung, the length of the syllables, and the file of origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "import IPython.display\n",
    "import pickle\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from PIL import Image\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avgn.segment_song.preprocessing import *\n",
    "import avgn.segment_song.wav_to_syllables as w2s\n",
    "import avgn.spectrogramming.spectrogramming as sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for Syllable Segmentation\n",
    "- Set to brown-headed cowbird recordings.\n",
    "- **TODO: UPDATE PARAMETERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/parameter_dictionaries/2019-08-12_15-17-23_dict.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-bc089858b98d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# save the dictionary so that we can reload it for recovering waveforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mdict_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/parameter_dictionaries/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mnow_string\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_dict.pickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/parameter_dictionaries/2019-08-12_15-17-23_dict.pickle'"
     ]
    }
   ],
   "source": [
    "syll_size = 128\n",
    "hparams = {\n",
    "    'species':'cb',\n",
    "    # filtering\n",
    "    'highcut':15000,\n",
    "    'lowcut':0,\n",
    "    \n",
    "    # spectrograms\n",
    "    'mel_filter': True, # should a mel filter be used?\n",
    "    'num_mels':syll_size, # how many channels to use in the mel-spectrogram\n",
    "    'num_freq':512, # how many channels to use in a spectrogram \n",
    "    'num_freq_final': syll_size, # how many channels to use in the resized spectrogram\n",
    "    'sample_rate':44100, # what rate are your WAVs sampled at?\n",
    "    'preemphasis':0.97, \n",
    "    'min_silence_for_spec': 0.5, #minimum length of silence for a spectrogram to be considered a good spectrogram\n",
    "    'frame_shift_ms':0.5, # step size for fft\n",
    "    'frame_length_ms':6, # frame length for fft\n",
    "    'min_level_db':-95, # minimum threshold db for computing spe \n",
    "    'spec_thresh_min': -40, # (db)\n",
    "    'spec_thresh_delta': 5, # (db) what \n",
    "    'ref_level_db':20, # reference db for computing spec\n",
    "    'sample_rate':44100, # sample rate of your data\n",
    "    'fmin': 300, # low frequency cutoff for mel filter\n",
    "    'fmax': None, # high frequency cutoff for mel filter\n",
    "    \n",
    "    # Vocal Envelope\n",
    "    'smoothing' : 'gaussian', # 'none', \n",
    "    'envelope_signal' : \"spectrogram\", # spectrogram or waveform, what to get the vocal envelope from\n",
    "    'gauss_sigma_s': .0001,\n",
    "    'FOI_min': 4, # minimum frequency of interest for vocal envelope (in terms of mel)\n",
    "    'FOI_max': 24, # maximum frequency of interest for vocal envelope (in terms of mel)\n",
    "    \n",
    "    # Silence Thresholding\n",
    "    'silence_threshold' : 0, # normalized threshold for silence\n",
    "    'min_len' : 5., # minimum length for a vocalization (fft frames)\n",
    "    'power_thresh': .3, # Threshold for which a syllable is considered to be quiet weak and is probably noise\n",
    "\n",
    "    # Syllabification\n",
    "    'min_syll_len_s' : 0.03, # minimum length for a syllable\n",
    "    'segmentation_rate': 0.0,#0.125, # rate at which to dynamically raise the segmentation threshold (ensure short syllables)\n",
    "    'threshold_max': 0.25,\n",
    "    'min_num_sylls': 20, # min number of syllables to be considered a bout\n",
    "    'slow_threshold':0.0,#0.02, # second slower threshold\n",
    "    'max_size_syll': syll_size, # the size of the syllable\n",
    "    'resize_samp_fr': int(syll_size*5.0), # (frames/s) the framerate of the syllable (in compressed spectrogram time components)\n",
    "    \n",
    "    # Sencond pass syllabification\n",
    "    'second_pass_threshold_repeats':50, # the number of times to repeat the second pass threshold\n",
    "    'ebr_min': 0.05, # expected syllabic rate (/s) low \n",
    "    'ebr_max':  0.2, # expected syllabic rate (/s) high \n",
    "    'max_thresh':  0.02, # maximum pct of syllabic envelope to threshold at in second pass\n",
    "    'thresh_delta':  0.005, # delta change in threshold to match second pass syllabification\n",
    "    'slow_threshold': 0.005, # starting threshold for second pass syllabification\n",
    "    \n",
    "    'pad_length' : syll_size, # length to pad spectrograms to \n",
    "    \n",
    "    # spectrogram inversion\n",
    "    'max_iters':200,\n",
    "    'griffin_lim_iters':60,\n",
    "    'power':1.5,\n",
    "\n",
    "    # thresholding out noise\n",
    "    'mel_noise_filt' : 0.15, # thresholds out low power noise in the spectrum - higher numbers will diminish inversion quality\n",
    "}\n",
    "\n",
    "globals().update(hparams)\n",
    "now_string = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") # this is used to identify this training instance\n",
    "\n",
    "# save the dictionary so that we can reload it for recovering waveforms\n",
    "dict_save = '../data/parameter_dictionaries/'+now_string+'_dict.pickle'\n",
    "with open(dict_save, 'wb') as f:\n",
    "    pickle.dump(hparams, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(dict_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "- Visualize segmentation algorithm on an example WAV file (ie. test).\n",
    "- Read data and runs bandpass filter on data.\n",
    "- Outputs vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_loc = '../data/cb_wavs/test_bird.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "csv_loc = '/'.join(wav_loc.split('/')[:-2] + ['csv'] + [wav_loc.split('/')[-1][:-4] + '.csv'])\n",
    "rate, data= wavfile.read(wav_loc)\n",
    "(bird,original_wav, start_time) = pd.read_csv(csv_loc, header=None).values[0]\n",
    "start_time = datetime.strptime(start_time, \"%Y-%m-%d_%H-%M-%S-%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bandpass filter (ie. frequency ranges)\n",
    "data = butter_bandpass_filter(data, lowcut, highcut, rate, order=2).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rate: ', rate, 'Time sung: ',start_time, 'Length:', len(data)/float(rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(20,4))\n",
    "plt.plot(data, color='black')\n",
    "ax.set_xlim((0, len(data)))\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play Song Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play back a sample of the song\n",
    "IPython.display.Audio(data=data[:rate*10], rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Spectrograms\n",
    "- Also outputs vocal envelope of spectogram.\n",
    "- Outputs visualization and statistics of the generated file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mel_basis = sg._build_mel_basis(hparams) # build a basis function if you are using a mel spectrogram\n",
    "# Generates the spectrogram and also thresholds out bad spectrograms (e.g. too noisy) - take a look at wav_to_syllables.py to determine if you want this\n",
    "spec, vocal_envelope, cur_spec_thresh, fft_time_idx, fft_rate = w2s.compute_spec_and_env((data/32768.).astype('float32'), start_time, hparams, rate, _mel_basis, verbose = True, mel_filter=mel_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the generated spectrogram\n",
    "plt.clf()\n",
    "start_time = fft_time_idx[0]\n",
    "stop_time = fft_time_idx[-1]\n",
    "fig, ax = plt.subplots(nrows=2,ncols=1, figsize=(80,6))\n",
    "\n",
    "ax[0].matshow(spec, interpolation=None, aspect='auto', # cmap=plt.cm.gray_r,\n",
    "                 cmap=plt.cm.viridis, origin='lower', extent=[start_time,stop_time,0,rate/2])\n",
    "ax[0].set_title('Spectrogram')\n",
    "\n",
    "ax[1].plot(vocal_envelope, color = 'red')\n",
    "ax[1].set_xlim([0,len(vocal_envelope)])\n",
    "ax[1].set_title('Vocal Envelope')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output some stats of our file\n",
    "print(''.join(['new Spectrogram Size: ',str(np.shape(spec))]))\n",
    "print(''.join(['Original Waveform Size: ',str(np.shape(data))]))\n",
    "print(''.join(['Length (s): ', str(len(data)/float(rate))]))\n",
    "print(''.join(['Original Sampling Rate (ms) : ', str(1./float(rate))]))\n",
    "print(''.join(['New Sampling Rate (ms): ', str(round((len(data)/float(rate))*1000 / float(np.shape(spec)[1]), 3))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Onsets/Offsets of Noise/Vocalization\n",
    "- Of Vocal Envelope, for syllabification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect onsets and offsets of vocal envelope\n",
    "# vocal_envelope =np.mean(norm(wav_spectrogram.T), axis =0)\n",
    "# vocal_envelope = norm(np.sum(spec, axis = 0))\n",
    "onsets, offsets = detect_onsets_offsets(vocal_envelope, \n",
    "      threshold = silence_threshold,\n",
    "      min_distance = 0.\n",
    "     )\n",
    "\n",
    "print(onsets[0:10], offsets[0:10], len(onsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fft_rate = len(data)/rate/float(np.shape(spec)[1])\n",
    "for i in range(second_pass_threshold_repeats):\n",
    "    onsets, offsets = w2s.second_pass_threshold(onsets, offsets, vocal_envelope,new_fft_rate, hparams)\n",
    "    #print(len(onsets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syllabification\n",
    "- Based on onsests and offsets.\n",
    "- Plot in histogram and remove short syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment into syllables based upon onset/offsets\n",
    "all_syllables, all_syllables_time_idx, syll_start = w2s.cut_syllables(onsets, offsets, spec, fft_time_idx, hparams)\n",
    "# Timing for when each syllable started (seconds)\n",
    "print(syll_start[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2s.plot_seg_spec(np.concatenate(np.array((onsets,offsets))), spec, fft_time_idx, vocal_envelope, 0, hparams)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_lengths = [np.shape(all_syllables[i])[1]/fft_rate for i in range(len(all_syllables))]\n",
    "print(len(syllable_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot how we've thresholded out syllables which are too short to be considered song\n",
    "print(np.min(syllable_lengths), np.max(syllable_lengths), np.mean(syllable_lengths))\n",
    "print(syllable_lengths[0:5])\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(16,3))\n",
    "_ = plt.hist(syllable_lengths,bins=50)\n",
    "ax.axvline(x=hparams['min_syll_len_s'], color = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short syllables\n",
    "all_syllables, all_syllables_time_idx, syll_start = w2s.threshold_syllables(all_syllables, all_syllables_time_idx, syll_start, min_syll_len_s, fft_rate, power_thresh = power_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Spectral Reconstruction on Syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:01.535309Z",
     "start_time": "2018-10-22T02:07:01.033823Z"
    }
   },
   "outputs": [],
   "source": [
    "syll_num = 10\n",
    "syll = all_syllables[syll_num]\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(4,4))\n",
    "ax.matshow(syll, origin='lower', aspect='auto')\n",
    "plt.show()\n",
    "print(np.shape(all_syllables[syll_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:01.672445Z",
     "start_time": "2018-10-22T02:07:01.592324Z"
    }
   },
   "outputs": [],
   "source": [
    "mel_inversion_filter = (_mel_basis.T / _mel_basis.sum(axis=1))\n",
    "mel_inversion_filter =np.nan_to_num(np.transpose(mel_inversion_filter / mel_inversion_filter.sum(axis=1)[:, np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:01.934405Z",
     "start_time": "2018-10-22T02:07:01.690909Z"
    }
   },
   "outputs": [],
   "source": [
    "# original wav\n",
    "IPython.display.Audio(data=data[int(all_syllables_time_idx[syll_num][0]):int(all_syllables_time_idx[syll_num][-1])], rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:02.116503Z",
     "start_time": "2018-10-22T02:07:01.942150Z"
    }
   },
   "outputs": [],
   "source": [
    "if mel_filter: \n",
    "    syll = np.dot(syll.T, mel_inversion_filter).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:02.275149Z",
     "start_time": "2018-10-22T02:07:02.121726Z"
    }
   },
   "outputs": [],
   "source": [
    "def rescale(X, out_min, out_max):\n",
    "    return out_min + (X - np.min(X)) * ((out_max - out_min) / (np.max(X) - np.min(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:02.415096Z",
     "start_time": "2018-10-22T02:07:02.297828Z"
    }
   },
   "outputs": [],
   "source": [
    "### adding some noise tends to improve reconstruction quality\n",
    "syll = rescale(syll, 0,1) +np.reshape(np.random.rand(np.prod(np.shape(syll)))*.25, np.shape(syll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:02.958349Z",
     "start_time": "2018-10-22T02:07:02.422535Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=1, figsize=(4,4))\n",
    "ax.matshow(syll, origin='lower', aspect='auto')\n",
    "plt.show()\n",
    "print(np.shape(syll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:06.980228Z",
     "start_time": "2018-10-22T02:07:02.971356Z"
    }
   },
   "outputs": [],
   "source": [
    "# test spectrogram inversion\n",
    "waveform = sg.inv_spectrogram(rescale(syll,.5,1), hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:07.090441Z",
     "start_time": "2018-10-22T02:07:06.988695Z"
    }
   },
   "outputs": [],
   "source": [
    "# play back a sample of the song\n",
    "IPython.display.Audio(data=waveform, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize and Pad Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:07.183469Z",
     "start_time": "2018-10-22T02:07:07.098255Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:07.297954Z",
     "start_time": "2018-10-22T02:07:07.190920Z"
    }
   },
   "outputs": [],
   "source": [
    "all_syllables_comp = copy.deepcopy(all_syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:07.398459Z",
     "start_time": "2018-10-22T02:07:07.309805Z"
    }
   },
   "outputs": [],
   "source": [
    "def sub_mode(syll):\n",
    "    freqs, bins = np.histogram(syll, bins = 100)\n",
    "    syll = syll - bins[np.argsort(freqs)][-1]#np.median(syll)#\n",
    "    syll[syll <0] = 0\n",
    "    return syll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:07.598428Z",
     "start_time": "2018-10-22T02:07:07.406177Z"
    }
   },
   "outputs": [],
   "source": [
    "# resize spectrogram\n",
    "all_syllables_comp = w2s.resize_spectrograms(all_syllables_comp, max_size = max_size_syll, resize_samp_fr = resize_samp_fr, fft_rate = fft_rate, n_freq= num_freq_final, pad_length=pad_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:07.693523Z",
     "start_time": "2018-10-22T02:07:07.606113Z"
    }
   },
   "outputs": [],
   "source": [
    "# set mode to 0\n",
    "#all_syllables_comp = [sub_mode(syll) for syll in all_syllables_comp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:07.845990Z",
     "start_time": "2018-10-22T02:07:07.701659Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# set mode to 0\n",
    "all_syllables_comp = [sub_mode(syll) for syll in all_syllables_comp]\n",
    "\n",
    "# 0 pade\n",
    "all_syllables_comp = np.array([w2s.pad_spectrogram(i, pad_length) for i in all_syllables_comp])\n",
    "all_syllables_comp = [(norm(i)*255).astype('uint8') for i in all_syllables_comp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test spectral reconstruction on syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:07.940745Z",
     "start_time": "2018-10-22T02:07:07.854008Z"
    }
   },
   "outputs": [],
   "source": [
    "syll = all_syllables_comp[syll_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:08.702239Z",
     "start_time": "2018-10-22T02:07:07.948649Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(6,3))\n",
    "_ = ax[0].hist(syll.flatten())\n",
    "ax[1].matshow(norm(syll.astype('float32')), origin='lower', aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:08.782221Z",
     "start_time": "2018-10-22T02:07:08.709713Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove edges\n",
    "syll_mean = np.mean(syll, axis=0)\n",
    "if syll_mean[0] == syll_mean[-1]:\n",
    "    sb = [i for i in np.arange(1, len(syll_mean)) if (syll_mean[i] == syll_mean[0]) and (syll_mean[i-1] == syll_mean[0])]\n",
    "    sb = [i for i in sb if i+1 not in sb][0], [i for i in sb if i-1 not in sb][-1]\n",
    "    syll = syll[:,sb[0]+1:sb[1]-1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:08.955587Z",
     "start_time": "2018-10-22T02:07:08.790535Z"
    }
   },
   "outputs": [],
   "source": [
    "if mel_filter:\n",
    "    resize_shape = (int((np.shape(syll)[1]/hparams['resize_samp_fr']) * (1000/hparams['frame_shift_ms'])), hparams['num_freq_final'])\n",
    "    syll = np.array(Image.fromarray(np.squeeze(syll)).resize(resize_shape, Image.ANTIALIAS))\n",
    "    syll = np.dot(syll.T, mel_inversion_filter).T\n",
    "else:\n",
    "    resize_shape = (int((np.shape(syll)[1]/hparams['resize_samp_fr']) * (1000/hparams['frame_shift_ms'])), hparams['num_freq'])\n",
    "    syll = np.array(Image.fromarray(np.squeeze(syll)).resize(resize_shape, Image.ANTIALIAS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:09.060801Z",
     "start_time": "2018-10-22T02:07:08.959014Z"
    }
   },
   "outputs": [],
   "source": [
    "### adding some noise tends to improve reconstruction quality\n",
    "syll = rescale(syll, .25,1) +np.reshape(np.random.rand(np.prod(np.shape(syll)))*.25, np.shape(syll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:09.934268Z",
     "start_time": "2018-10-22T02:07:09.074021Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(6,3))\n",
    "_ = ax[0].hist(syll.flatten())\n",
    "ax[1].matshow(norm(syll.astype('float32')), origin='lower', aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:14.110154Z",
     "start_time": "2018-10-22T02:07:09.942014Z"
    }
   },
   "outputs": [],
   "source": [
    "# test spectrogram inversion\n",
    "waveform = sg.inv_spectrogram(rescale(syll, .5,1), hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:14.242444Z",
     "start_time": "2018-10-22T02:07:14.117507Z"
    }
   },
   "outputs": [],
   "source": [
    "# play back a sample of the song\n",
    "IPython.display.Audio(data=waveform, rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot computed syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:15.002047Z",
     "start_time": "2018-10-22T02:07:14.250277Z"
    }
   },
   "outputs": [],
   "source": [
    "w2s.plt_all_syllables(all_syllables_comp, num_freq_final,  max_rows = 8, max_sylls = 100, width = 1400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:24.767799Z",
     "start_time": "2018-10-22T02:07:15.010531Z"
    }
   },
   "outputs": [],
   "source": [
    "# This takes a while to run... we're visualizing the whole pipeline on this song\n",
    "w2s.plot_pipeline(data, vocal_envelope,spec.T,onsets,offsets, all_syllables_comp,rate,all_syllables_time_idx,syllable_lengths, figsize = (100,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run through in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:24.856376Z",
     "start_time": "2018-10-22T02:07:24.775977Z"
    }
   },
   "outputs": [],
   "source": [
    "# find the data bird folders\n",
    "dataset_location = '../../../data/bf_wav/*'\n",
    "bird_folders = glob(dataset_location)\n",
    "bird_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:07:24.962337Z",
     "start_time": "2018-10-22T02:07:24.869170Z"
    }
   },
   "outputs": [],
   "source": [
    "data_output_location = '../../../data/bf_song_syllables/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:21:35.520880Z",
     "start_time": "2018-10-22T02:21:35.075429Z"
    }
   },
   "outputs": [],
   "source": [
    "skip_existing = False # skip already segmented birds \n",
    "parallel = True # run through WAVs in parallel\n",
    "n_jobs = 10 # how many threads to use if parallel is true\n",
    "verbosity = 5 # how verbose to make the output of the parallelization (higher = more, 0 = none, >50 output is sent to std.out)\n",
    "nex = 10 # how many example wavs to plot\n",
    "save=True\n",
    "visualize = False # visualize the output of the algorithm for optimizing parameters\n",
    "if visualize==True: \n",
    "    save=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan \n",
    "from skimage.transform import resize\n",
    "import umap\n",
    "import time \n",
    "import seaborn as sns\n",
    "\n",
    "def plot_with_labels(data, labels, title = '', ax = None, figsize = (9,9)):\n",
    "    palette = sns.color_palette('husl', len(np.unique(labels)))\n",
    "    labs_to_numbers_dict = {l:i for i,l in enumerate(np.unique(labels))}\n",
    "    np.random.shuffle(palette)\n",
    "    colors = [palette[labs_to_numbers_dict[x]] if x >= 0 else (0.75, 0.75, 0.75) for x in np.array(labels)]\n",
    "\n",
    "    if not ax: fig, ax= plt.subplots(nrows=1,ncols=1,figsize=figsize)\n",
    "    ax.scatter(data.T[0], data.T[1],\n",
    "               color=colors, alpha = 1, linewidth= 0, s=5)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "    if not ax: plt.show()\n",
    "        \n",
    "def cluster_data(data, algorithm, args, kwds, verbose = True):\n",
    "    \"\"\" Cluster data using arbitrary clustering algoritm in sklearn\n",
    "    \"\"\"\n",
    "    # Function modified from HDBSCAN python package website\n",
    "    start_time = time.time()\n",
    "    labels = algorithm(*args, **kwds).fit_predict(data)\n",
    "    end_time = time.time()\n",
    "    if verbose: print('Clustering took {:.2f} s'.format(end_time - start_time))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:42:31.007993Z",
     "start_time": "2018-10-22T02:21:35.685016Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "key_list = (\n",
    "            'all_bird_wav_file', # Wav file (bout_raw) that the syllable came from\n",
    "            'all_bird_syll', # spectrogram of syllable\n",
    "            'all_bird_syll_start', # time that this syllable occured\n",
    "            'all_bird_t_rel_to_file', # time relative to bout file that this \n",
    "            'all_bird_syll_lengths' # length of the syllable\n",
    "           ) \n",
    "\n",
    "for bird_folder in tqdm(bird_folders):\n",
    "    bird_name = bird_folder.split('/')[-1]\n",
    "    print(bird_name)\n",
    "\n",
    "    # prepare the data folder\n",
    "    if not os.path.exists(''.join([data_output_location,species,'/'])):\n",
    "        os.makedirs(''.join([data_output_location,species,'/'])) \n",
    "    output_filename = ''.join([data_output_location,species,'/',bird_name,'_'+str(syll_size)+'.hdf5'])\n",
    "    if os.path.exists(output_filename) and skip_existing:\n",
    "        print('%s already complete, skipping' % (bird_name))\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # initialize lists of bird information\n",
    "    bird_data = {key : [] for key in key_list}\n",
    "    wav_list = glob(bird_folder + '/wavs/*.wav')\n",
    "    print(bird_name, len(wav_list))\n",
    "    \n",
    "    if visualize==True: wav_list = wav_list[:nex]\n",
    "\n",
    "    # Syllabify/create dataset\n",
    "    if parallel:\n",
    "        with Parallel(n_jobs=n_jobs, verbose=verbosity) as parallel:\n",
    "            bird_data_packed = parallel(\n",
    "                delayed(w2s.process_bout)(wav_file,_mel_basis,hparams=hparams,submode=True, visualize = visualize) \n",
    "                     for wav_file in tqdm(wav_list))\n",
    "    else:\n",
    "        bird_data_packed = [w2s.process_bout(wav_file,_mel_basis, hparams=hparams,submode=True, visualize = visualize) for wav_file in tqdm(wav_list)]\n",
    "    \n",
    "    if np.sum([i[0] != [] for i in bird_data_packed]) == 0:\n",
    "        print('Bird had no good bouts')\n",
    "        continue\n",
    "        \n",
    "    for dtype, darray in zip(key_list, list(zip(*bird_data_packed))):\n",
    "            [bird_data[dtype].extend(element) for element in darray] # flatten and clear darray -> bird_data[dtype]\n",
    "            bird_data[dtype] = np.array(bird_data[dtype])\n",
    "\n",
    "    # reformat bird syllables\n",
    "    print('len dataset: ', len(bird_data['all_bird_syll_lengths']))\n",
    "    \n",
    "    # embed\n",
    "    x = bird_data['all_bird_syll'].reshape((len(bird_data['all_bird_syll']), syll_size,syll_size))\n",
    "\n",
    "    x_small = [resize(i, [16,16]) for i in tqdm(x)]\n",
    "    x_small = np.array(x_small).reshape((len(x_small), np.prod(np.shape(x_small)[1:])))\n",
    "    x_small = [(i*255).astype('uint8') for i in x_small]\n",
    "\n",
    "    z = umap.UMAP(\n",
    "        n_neighbors=30,\n",
    "        #min_dist=0.0,\n",
    "        n_components=2,\n",
    "        random_state=42,\n",
    "    ).fit_transform(x_small)\n",
    "    #label\n",
    "    labels = cluster_data(z,\n",
    "          hdbscan.HDBSCAN,\n",
    "          (),\n",
    "          {'min_cluster_size':100,  'min_samples':1},\n",
    "         verbose = True)\n",
    "    # plot clusters\n",
    "    plot_with_labels(np.array(list(z)), labels, figsize=(20,20))\n",
    "    plt.show()\n",
    "\n",
    "    if save:\n",
    "        w2s.save_dataset(output_filename, \n",
    "                     bird_data['all_bird_syll'], \n",
    "                     bird_data['all_bird_syll_start'].astype('object'),\n",
    "                     bird_data['all_bird_syll_lengths'], \n",
    "                     bird_data['all_bird_wav_file'].astype('object'),\n",
    "                     bird_data['all_bird_t_rel_to_file'],\n",
    "                     bird_name\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
